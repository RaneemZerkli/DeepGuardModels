{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d2c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d806a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('newdeepfakedetectiontextmodel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa210ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd13cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, texts):\n",
    "  \"\"\"Tokenizes a list of text strings using the given BERT tokenizer.\n",
    "\n",
    "  Args:\n",
    "    tokenizer: A BERT tokenizer.\n",
    "    texts: A list of text strings.\n",
    "\n",
    "  Returns:\n",
    "    A list of tokenized sentences, where each sentence is a list of token IDs.\n",
    "  \"\"\"\n",
    "\n",
    "  tokenized_texts = []\n",
    "  for text in texts:\n",
    "    # Fix the error by removing the `return_tensors='pt'` argument.\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128)['input_ids']\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "  return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4edaba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted class: machine generated [[0.00241945]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "text=\" Define your model architecture. The architecture of your BiLSTM model will depend on the specific task that you are trying to solve. For example, if you are trying to classify text, you will need to add a classification layer to the end of your model. \"\n",
    "text_p = [text]\n",
    "x2 = tokenize_data(tokenizer, text_p)\n",
    "\n",
    "# Make a prediction\n",
    "prediction_input = np.array(x2).reshape(-1)\n",
    "prediction_input=pad_sequences([prediction_input],128)\n",
    "\n",
    "\n",
    "output = model.predict(prediction_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if output > 0.5:\n",
    "    predicted_class = 'human written'\n",
    "else:\n",
    "    predicted_class = 'machine generated'\n",
    "\n",
    "# Print the predicted class\n",
    "print('Predicted class:', predicted_class,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8cc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
