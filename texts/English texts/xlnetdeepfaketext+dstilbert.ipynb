{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafedabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f174be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('deepfake3text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa07ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc041255c124bbbb608f13dbd651be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import  XLNetTokenizer\n",
    "\n",
    "# Load the base-cased XLNet model\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dc1233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c6a521a4e34b1d9baded700138417e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60ba84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "text = df['Inputs']\n",
    "label = df['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69123250",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(text, label, test_size=0.25, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab0b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, texts):\n",
    "  \"\"\"Tokenizes a list of text strings using the given BERT tokenizer.\n",
    "\n",
    "  Args:\n",
    "    tokenizer: A BERT tokenizer.\n",
    "    texts: A list of text strings.\n",
    "\n",
    "  Returns:\n",
    "    A list of tokenized sentences, where each sentence is a list of token IDs.\n",
    "  \"\"\"\n",
    "\n",
    "  tokenized_texts = []\n",
    "  for text in texts:\n",
    "    # Fix the error by removing the `return_tensors='pt'` argument.\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128)['input_ids']\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "  return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2459047",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =tokenize_data(tokenizer, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a987421",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 =tokenize_data(tokenizer, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f226cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "x_train=pad_sequences(x)\n",
    "x_val=pad_sequences(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "345b9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=model.config.vocab_size,\n",
    "    output_dim=model.config.hidden_size,\n",
    "    weights=[model.get_input_embeddings().weight.detach()],\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a05cdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7990eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "478e8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 6s 13ms/step - loss: 0.2184 - accuracy: 0.9156\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0983 - accuracy: 0.9694\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0910 - accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0893 - accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0641 - accuracy: 0.9806\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0595 - accuracy: 0.9784\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0585 - accuracy: 0.9812\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0532 - accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0625 - accuracy: 0.9797\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0519 - accuracy: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4b8f92bc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10,steps_per_epoch=100,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7f6f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0491 - accuracy: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04909510165452957, 0.9865599870681763]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val,y_val,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c9c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted class: machine generated [[0.00593289]]\n"
     ]
    }
   ],
   "source": [
    "text=\" The University of Chicago is thomen in PublicAffairs Council project. Learn more about the program.In order to become\"\n",
    "text_p = [text]\n",
    "x2 = tokenize_data(tokenizer, text_p)\n",
    "\n",
    "# Make a prediction\n",
    "prediction_input = np.array(x2).reshape(-1)\n",
    "prediction_input=pad_sequences([prediction_input],128)\n",
    "\n",
    "\n",
    "output = model.predict(prediction_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if output > 0.5:\n",
    "    predicted_class = 'human written'\n",
    "else:\n",
    "    predicted_class = 'machine generated'\n",
    "\n",
    "# Print the predicted class\n",
    "print('Predicted class:', predicted_class,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "119abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('xlnet_english_deepfakedetection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5474b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 768)         24576000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               459264    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,035,393\n",
      "Trainable params: 459,393\n",
      "Non-trainable params: 24,576,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4acf8a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a13efafe4614a86ada5ee7d70ed32c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc665645",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =tokenize_data(tokenizer, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81498085",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 =tokenize_data(tokenizer, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24e0926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "x_train=pad_sequences(x)\n",
    "x_val=pad_sequences(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b5976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=model.config.vocab_size,\n",
    "    output_dim=model.config.hidden_size,\n",
    "    weights=[model.get_input_embeddings().weight.detach()],\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f7155a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb25d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55da2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 2s 13ms/step - loss: 0.2551 - accuracy: 0.9141\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1668 - accuracy: 0.9503\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.1375 - accuracy: 0.9566\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1642 - accuracy: 0.9484\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1425 - accuracy: 0.9594\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1389 - accuracy: 0.9588\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1321 - accuracy: 0.9600\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1190 - accuracy: 0.9663\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1235 - accuracy: 0.9594\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1045 - accuracy: 0.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f71204b610>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10,steps_per_epoch=100,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84355ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0942 - accuracy: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0941619798541069, 0.9707599878311157]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val,y_val,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e051e30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted class: machine generated [[0.00894152]]\n"
     ]
    }
   ],
   "source": [
    "text=\" Make sure you have the latest version of PRAW installed, and the code above should work. If you encounter any issues, refer to the PRAW documentation or the specific version's documentation for guidance.\"\n",
    "text_p = [text]\n",
    "x2 = tokenize_data(tokenizer, text_p)\n",
    "\n",
    "# Make a prediction\n",
    "prediction_input = np.array(x2).reshape(-1)\n",
    "prediction_input=pad_sequences([prediction_input],128)\n",
    "\n",
    "\n",
    "output = model.predict(prediction_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if output > 0.5:\n",
    "    predicted_class = 'human written'\n",
    "else:\n",
    "    predicted_class = 'machine generated'\n",
    "\n",
    "# Print the predicted class\n",
    "print('Predicted class:', predicted_class,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a51485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('distbertdeepfakedetection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1b10df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_arabic(text):\n",
    "    arabic_regex = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    return arabic_regex.search(text) != None\n",
    "\n",
    "def is_english(text):\n",
    "    english_regex = re.compile(r'[a-zA-Z]+')\n",
    "    return english_regex.search(text) != None\n",
    "\n",
    "def check_language(text):\n",
    "    if is_arabic(text):\n",
    "        print(\"The text is Arabic.\")\n",
    "    elif is_english(text):\n",
    "        print(\"The text is English.\")\n",
    "    else:\n",
    "        print(\"The text is neither Arabic nor English.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4abc751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3b6a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is Arabic.\n"
     ]
    }
   ],
   "source": [
    "check_language(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
