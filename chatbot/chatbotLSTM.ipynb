{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce25827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbfd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060e4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8cb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Input,Embedding,Dense,GlobalMaxPooling1D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c219991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c650598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents11 (2).json') as intents:\n",
    "    data=json.load(intents)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8e8aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'machine_learning',\n",
       "   'patterns': ['What is Machine Learning?', 'Explain Machine Learning.'],\n",
       "   'responses': ['Machine Learning (ML) is a field of artificial intelligence that focuses on developing algorithms and models that allow computer systems to learn and make predictions or decisions without being explicitly programmed. Key points about Machine Learning include:',\n",
       "    '- ML algorithms learn from data and identify patterns to make predictions or take actions.',\n",
       "    '- Supervised learning uses labeled data to train models, while unsupervised learning discovers patterns in unlabeled data.',\n",
       "    '- Common ML algorithms include decision trees, support vector machines, and neural networks.',\n",
       "    '- ML finds applications in various domains, such as image recognition, natural language processing, and recommender systems.',\n",
       "    '- The performance of ML models is evaluated using metrics like accuracy, precision, recall, and F1 score.']},\n",
       "  {'tag': 'deep_learning',\n",
       "   'patterns': ['What is Deep Learning?', 'Explain Deep Learning.'],\n",
       "   'responses': ['Deep Learning (DL) is a subfield of machine learning that focuses on training artificial neural networks with multiple layers to learn and make complex decisions or predictions. Key points about Deep Learning include:',\n",
       "    '- DL models, known as deep neural networks, can automatically learn hierarchical representations of data.',\n",
       "    '- Deep neural networks are composed of interconnected layers of artificial neurons that process and transform data.',\n",
       "    '- DL has achieved remarkable success in various tasks, including image and speech recognition, natural language processing, and generative modeling.',\n",
       "    '- Convolutional Neural Networks (CNNs) are commonly used in computer vision tasks, while Recurrent Neural Networks (RNNs) are used for sequential data.',\n",
       "    '- DL models require large amounts of labeled data and significant computational resources for training.']},\n",
       "  {'tag': 'nlp',\n",
       "   'patterns': ['What is Natural Language Processing?',\n",
       "    'Explain Natural Language Processing.'],\n",
       "   'responses': ['Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language. NLP enables computers to understand, interpret, and generate human language. Key points about Natural Language Processing include:',\n",
       "    '- NLP encompasses tasks such as text classification, sentiment analysis, machine translation, and question-answering systems.',\n",
       "    '- NLP techniques involve tokenization, part-of-speech tagging, named entity recognition, syntactic parsing, and semantic analysis.',\n",
       "    '- NLP models leverage machine learning and deep learning algorithms, such as recurrent neural networks (RNNs) and transformers.',\n",
       "    '- NLP applications include chatbots, virtual assistants, language translation, information retrieval, and text summarization.',\n",
       "    '- NLP faces challenges such as ambiguity, context understanding, and cultural/language variations.']},\n",
       "  {'tag': 'cv',\n",
       "   'patterns': ['What is Computer Vision?', 'Explain Computer Vision.'],\n",
       "   'responses': ['Computer Vision (CV) is a field of artificial intelligence that focuses on enabling machines to analyze, understand, and interpret visual information from images or videos. Key points about Computer Vision include:',\n",
       "    '- CV algorithms aim to replicate human vision capabilities, including object recognition, image segmentation, and scene understanding.',\n",
       "    '- CV tasks include face recognition, object detection, image classification, image generation, and video analysis.',\n",
       "    '- Deep learning techniques, such as convolutional neural networks (CNNs), have significantly advanced the field of computer vision.',\n",
       "    '- CV finds applications in autonomous vehicles, surveillance systems, medical imaging, augmented reality, and robotics.',\n",
       "    '- CV faces challenges such as occlusion, illumination variations, viewpoint changes, and object scale differences.']},\n",
       "  {'tag': 'deepfake_audio',\n",
       "   'patterns': ['What are Deepfake Audios?', 'Explain Deepfake Audios.'],\n",
       "   'responses': ['Deepfake Audios refer to manipulated or synthetic audio clips that are created using artificial intelligence techniques, particularly deep learning. Key points about Deepfake Audios include:',\n",
       "    '- Deepfake Audios are generated by training deep learning models on large datasets of real audio samples.',\n",
       "    \"- These models learn the characteristics of a person's voice and can generate speech that mimics their voice patterns and intonations.\",\n",
       "    '- Deepfake Audios have raised concerns about potential misuse, such as impersonation, misinformation, and fraud.',\n",
       "    '- The detection of Deepfake Audios is an active area of research, aiming to develop techniques to identify and authenticate audio recordings.']},\n",
       "  {'tag': 'deepfake_text',\n",
       "   'patterns': ['What are Deepfake Texts?', 'Explain Deepfake Texts.'],\n",
       "   'responses': ['Deepfake Texts refer to artificially generated or manipulated text content that appears to be written by a human but is actually created using AItechniques. Key points about Deepfake Texts include:',\n",
       "    '- Deepfake Texts can be generated using natural language processing (NLP) models, such as language models or transformers.',\n",
       "    '- These models learn from large amounts of text data and can generate coherent and contextually relevant text that mimics human writing.',\n",
       "    '- Deepfake Texts can be used for various purposes, including generating fake news, spreading disinformation, or impersonating others.',\n",
       "    '- Detecting Deepfake Texts is a challenging task, and researchers are developing techniques to identify and mitigate the spread of fake text content.']},\n",
       "  {'tag': 'deepfake_video',\n",
       "   'patterns': ['What are Deepfake Videos?', 'Explain Deepfake Videos.'],\n",
       "   'responses': ['Deepfake Videos are manipulated or synthetic videos that are created using deep learning techniques. Key points about Deepfake Videos include:',\n",
       "    '- Deepfake Videos involve replacing or altering the faces and actions of individuals in existing videos using AI algorithms.',\n",
       "    '- Deep learning models are trained on large datasets of real videos to learn the facial features and expressions of individuals.',\n",
       "    '- These models can then generate realistic-looking videos that superimpose the face of one person onto another person in the original video.',\n",
       "    '- Deepfake Videos have raised concerns about the potential for misinformation, identity theft, and privacy violations.',\n",
       "    '- Deepfake Video detection techniques are being developed to identify forged or manipulated videos and mitigate their harmful effects.']},\n",
       "  {'tag': 'deepfake_image',\n",
       "   'patterns': ['What are Deepfake Images?', 'Explain Deepfake Images.'],\n",
       "   'responses': ['Deepfake Images are synthetic or manipulated images that are created using deep learning techniques. Key points about Deepfake Images include:',\n",
       "    '- Deepfake Images involve generating or altering images to create realistic or fake visuals using AI algorithms.',\n",
       "    '- Deep learning models are trained on large datasets of real images to learn the visual features and patterns.',\n",
       "    '- These models can then generate new images or modify existing images to manipulate the appearance of individuals or objects.',\n",
       "    '- Deepfake Images can be used for various purposes, including creating fake identities, altering photographs, or generating realistic visual effects.',\n",
       "    '- Detecting Deepfake Images is a challenging task, and researchers are developing techniques to identify and authenticate digital images.']},\n",
       "  {'tag': 'deepfake_detection_audio',\n",
       "   'patterns': ['How can Deepfake Audios be detected?',\n",
       "    'Detecting Deepfake Audios'],\n",
       "   'responses': ['Detecting Deepfake Audios is an active area of research, and various techniques are being developed to identify manipulated or synthetic audio recordings. Some approaches for Deepfake Audio detection include:',\n",
       "    '- Analysis of audio artifacts or inconsistencies that may occur due to the generation process.',\n",
       "    '- Comparison of voice characteristics, such as pitch, cadence, and pronunciation, with known reference samples.',\n",
       "    '- Utilizing machine learning algorithms to detect patterns or anomalies in the audio signals.',\n",
       "    '- Incorporating audio forensics techniques, such as spectrogram analysis or watermarking, to detect potential manipulations.',\n",
       "    '- Collaboration between researchers, industry experts, and audio professionals to develop robust detection methods.']},\n",
       "  {'tag': 'deepfake_detection_text',\n",
       "   'patterns': ['How can Deepfake Texts be detected?',\n",
       "    'Detecting Deepfake Texts'],\n",
       "   'responses': ['Detecting Deepfake Texts is a challenging task, but researchers are developing techniques to identify artificially generated or manipulated text content. Some approaches for Deepfake Text detection include:',\n",
       "    '- Analyzing linguistic patterns, grammar errors, or inconsistencies that may indicate machine-generated text.',\n",
       "    '- Leveraging natural language processing (NLP) models to detect anomalies in the language structure or style.',\n",
       "    '- Using statistical analysis or machine learning algorithms to identify patterns or features that differentiate human-written text from deepfake text.',\n",
       "    '- Building large-scale labeled datasets of real and fake text samples for training detection models.',\n",
       "    '- Development of collaborative efforts and shared resources among researchers and industry stakeholders to improve detection capabilities.']},\n",
       "  {'tag': 'deepfake_detection_video',\n",
       "   'patterns': ['How can Deepfake Videos be detected?',\n",
       "    'Detecting Deepfake Videos'],\n",
       "   'responses': ['Detecting Deepfake Videos is an active area of research, and several approaches are being explored to identify manipulated or synthetic video content. Some techniques for Deepfake Video detection include:',\n",
       "    '- Analysis of visual artifacts or inconsistencies that may be introduced during the deepfake generation process.',\n",
       "    '- Utilizing facial recognition algorithms to compare facial features and expressions with known reference samples.',\n",
       "    '- Applying deep learning models to detect anomalies or inconsistencies in the visual content or motion patterns.',\n",
       "    '- Incorporating video forensics techniques, such as analysis of compression artifacts or sensor noise, to identify potential manipulations.',\n",
       "    '- Collaboration between researchers, computer vision experts, and industry professionals to develop robust and reliable detection methods.']},\n",
       "  {'tag': 'deepfake_detection_image',\n",
       "   'patterns': ['How can Deepfake Images be detected?',\n",
       "    'Detecting Deepfake Images'],\n",
       "   'responses': ['Detecting Deepfake Images is a challenging task, but researchers are actively developing techniques to identify manipulated or synthetic images. Some approaches for Deepfake Image detection include:',\n",
       "    '- Analyzing inconsistencies or artifacts in the visual content, such as unusual pixel patterns or unnatural lighting.',\n",
       "    '- Using image forensics techniques to detect traces of tampering, such as detection of duplicated regions or inconsistencies in metadata.',\n",
       "    '- Leveraging deep learning models trained on large datasets of real and fake images to differentiate between authentic and manipulated visuals.',\n",
       "    '- Employing reverse image search or similarity analysis to identify if the image has been altered or reused from other sources.',\n",
       "    '- Collaborating with experts in computer vision, image processing, and forensic analysis to enhance detection capabilities and develop countermeasures.']},\n",
       "  {'tag': 'data_generation',\n",
       "   'patterns': ['What is data generation in Artificial Intelligence?',\n",
       "    'How is data generated for AI?'],\n",
       "   'responses': ['Data generation in Artificial Intelligence refers to the process of creating or augmenting datasets for training AI models. Some methods of data generation include:',\n",
       "    '- Manual data collection and annotation by human experts.',\n",
       "    '- Synthetic data generation using algorithms or simulations.',\n",
       "    '- Data augmentation techniques, such as image rotation, scaling, and flipping.',\n",
       "    '- Transfer learning, where pre-existing datasets are used to train models on related tasks.',\n",
       "    '- Active learning, where models select and query humans for relevant data to improve their performance.',\n",
       "    '- Reinforcement learning, where AI agents generate data through interactions with their environment.',\n",
       "    '- Crowdsourcing platforms that involve human contributions for data labeling and verification.',\n",
       "    '  In addition to data generation, AI encompasses various advanced technologies and applications:',\n",
       "    ' Deepfake:',\n",
       "    '   - Definition: Deepfake is a technique that uses deep learning algorithms to create realistic fake content, often in the form of videos or images, by manipulating or synthesizing existing media.',\n",
       "    '   - Example: Deepfake technology can be used to create videos that appear to feature individuals saying or doing things they never did, raising concerns about misinformation and privacy.',\n",
       "    ' Machine Learning:',\n",
       "    '   - Definition: Machine Learning (ML) is a subset of AI that involves the development of algorithms and models that enable computers to learn patterns and make predictions or decisions without explicit programming.',\n",
       "    '   - Example: An email spam filter is an application of machine learning, where the system learns to identify and filter out spam based on patterns in the data.',\n",
       "    '. Deep Learning:',\n",
       "    '   - Definition: Deep Learning is a subfield of machine learning that focuses on neural networks with multiple layers (deep neural networks). It is inspired by the structure and function of the human brain.',\n",
       "    '   - Example: Image recognition tasks, such as identifying objects in photos, often utilize deep learning models like convolutional neural networks (CNNs) for high accuracy.',\n",
       "    'These examples showcase the diverse applications of AI, from generating synthetic data for training models to addressing complex challenges through advanced techniques like deep learning and combating issues like deepfake.']},\n",
       "  {'tag': 'training_model',\n",
       "   'patterns': ['What is Model Training?', 'How does model training work?'],\n",
       "   'responses': ['Model Training refers to the process of teaching a machine learning or deep learning model to recognize patterns and make accurate predictions by adjusting its internal parameters. Key points about Model Training include:',\n",
       "    '- During training, the model learns from labeled examples by iteratively adjusting its parameters to minimize the difference between predicted outputs and target outputs.',\n",
       "    '- The training process involves feeding the model input data, computing predictions, comparing them with the actual labels, and updating the model using optimization algorithms (e.g., gradient descent).',\n",
       "    \"- The model's performance is evaluated using validation data, and various metrics (e.g., accuracy, loss) are used to measure its effectiveness.\",\n",
       "    '- Training a model requires defining its architecture, selecting appropriate loss functions, and choosing hyperparameters (e.g., learning rate, batch size).',\n",
       "    '- Model training can be computationally intensive and may require specialized hardware (e.g., GPUs) for faster processing.']},\n",
       "  {'tag': 'fine_tuning',\n",
       "   'patterns': ['What is Model Fine-Tuning?', 'How does fine-tuning work?'],\n",
       "   'responses': ['Fine-Tuning, also known as transfer learning, is a technique used to adapt a pre-trained model to a new task or dataset. Key points about Fine-Tuning include:',\n",
       "    '- Fine-Tuning leverages the knowledge and learned features of a model trained on a large-scale dataset to improve performance on a specific task or smaller dataset.',\n",
       "    '- Typically, the pre-trained model is taken as a starting point, and only the final layers or specific parts of the model are modified or retrained.',\n",
       "    \"- Fine-Tuning can save time and computational resources by utilizing the pre-trained model's knowledge, especially when the new dataset is limited.\",\n",
       "    '- It is common to freeze early layers, which capture general features, and update later layers, which capture task-specific features.',\n",
       "    '- Fine-Tuning is widely used in various domains, such as computer vision and natural language processing, to achieve state-of-the-art performance.']},\n",
       "  {'tag': 'zero_shot_learning',\n",
       "   'patterns': ['What is Zero-Shot Learning?',\n",
       "    'How does zero-shot learning work?'],\n",
       "   'responses': ['Zero-Shot Learning is a machine learning paradigm that enables models to recognize and classify objects or concepts without requiring training examples for each class. Key points about Zero-Shot Learning include:',\n",
       "    '- In traditional supervised learning, models are trained on labeled data for all possible classes. In contrast, Zero-Shot Learning allows models to generalize to unseen classes.',\n",
       "    '- Zero-Shot Learning leverages auxiliary information, such as class attributes or semantic embeddings, to bridge the gap between seen and unseen classes.',\n",
       "    '- During training, the model learns to associate class attributes with visual features, enabling it to make predictions for unseen classes based on attribute information.',\n",
       "    '- Zero-Shot Learning finds applications in scenarios where it is impractical or costly to obtain labeled data for all classes.',\n",
       "    '- Ongoing research focuses on improving the robustness and performance of Zero-Shot Learning algorithms for real-world applications.']},\n",
       "  {'tag': 'ai_overview',\n",
       "   'patterns': ['What is Artificial Intelligence?',\n",
       "    'Explain the concept of AI.'],\n",
       "   'responses': ['Artificial Intelligence (AI) is a branch of computer science that aims to create machines capable of intelligent behavior. It involves the development of algorithms and models that enable computers to perform tasks that typically require human intelligence.',\n",
       "    'In the realm of AI, data generation is crucial for training models. Some methods include:',\n",
       "    '- Manual data collection and annotation by human experts.',\n",
       "    '- Synthetic data generation using algorithms or simulations.',\n",
       "    '- Data augmentation techniques, such as image rotation, scaling, and flipping.',\n",
       "    '- Transfer learning, where pre-existing datasets are used to train models on related tasks.',\n",
       "    '- Active learning, where models select and query humans for relevant data to improve their performance.',\n",
       "    '- Reinforcement learning, where AI agents generate data through interactions with their environment.',\n",
       "    '- Crowdsourcing platforms that involve human contributions for data labeling and verification.',\n",
       "    'Additionally, AI encompasses advanced technologies such as:',\n",
       "    ' Deepfake:',\n",
       "    '   - Definition: Deepfake is a technique that uses deep learning algorithms to create realistic fake content, often in the form of videos or images, by manipulating or synthesizing existing media.',\n",
       "    '   - Example: Deepfake technology can be used to create videos that appear to feature individuals saying or doing things they never did, raising concerns about misinformation and privacy.',\n",
       "    ' Machine Learning:',\n",
       "    '   - Definition: Machine Learning (ML) is a subset of AI that involves the development of algorithms and models that enable computers to learn patterns and make predictions or decisions without explicit programming.',\n",
       "    '   - Example: An email spam filter is an application of machine learning, where the system learns to identify and filter out spam based on patterns in the data.',\n",
       "    ' Deep Learning:',\n",
       "    '   - Definition: Deep Learning is a subfield of machine learning that focuses on neural networks with multiple layers (deep neural networks). It is inspired by the structure and function of the human brain.',\n",
       "    '   - Example: Image recognition tasks, such as identifying objects in photos, often utilize deep learning models like convolutional neural networks (CNNs) for high accuracy.',\n",
       "    'These examples showcase the diverse applications of AI, from generating synthetic data for training models to addressing complex challenges through advanced techniques like deep learning and combating issues like deepfake.']},\n",
       "  {'tag': 'neural_networks',\n",
       "   'patterns': ['What are neural networks?', 'How do neural networks work? '],\n",
       "   'responses': [\"Neural networks are a computational model inspired by the human brain's structure and functioning. They consist of interconnected nodes, or artificial neurons, organized into layers. In a neural network, information is processed through these layers, and the connections between neurons are adjusted during training to enable the network to learn and make predictions. Neural networks are a fundamental component of deep learning.\"]},\n",
       "  {'tag': 'reinforcement_learning',\n",
       "   'patterns': ['What is reinforcement learning?',\n",
       "    'How does reinforcement learning work in AI?'],\n",
       "   'responses': ['Reinforcement learning is a type of machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or punishments based on the actions it takes. The goal of reinforcement learning is to learn a policy that maximizes the cumulative reward over time, allowing the agent to make optimal decisions in various scenarios.',\n",
       "    'In AI, reinforcement learning involves an agent, a set of actions it can take, an environment it interacts with, and a reward signal. The agent learns by exploring the environment, taking actions, and receiving feedback in the form of rewards or penalties. Notable applications of reinforcement learning include game playing, robotics, and autonomous systems.']},\n",
       "  {'tag': 'ml_learning_types',\n",
       "   'patterns': ['What is supervised learning and unsupervised learning ?',\n",
       "    'what is the difference between supervised learning and unsupervised learning '],\n",
       "   'responses': ['Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset. In this setup, each input data point is associated with a corresponding output or label. The algorithm learns to map input features to the correct output by generalizing patterns from the provided labeled examples. The goal is to make accurate predictions on new, unseen data while  Unsupervised learning is a machine learning paradigm where the algorithm is given unlabeled data. The objective is to find patterns, structures, or relationships within the data without explicit guidance on the output. Common tasks in unsupervised learning include clustering, where similar data points are grouped together, and dimensionality reduction, which involves reducing the number of input features while preserving essential information.']},\n",
       "  {'tag': 'general_conversation',\n",
       "   'patterns': ['How are you?', 'How does your day look?'],\n",
       "   'responses': [\"I'm just a computer program, so I don't have feelings, but thanks for asking!\",\n",
       "    \"Sure, here's a joke: Why don't scientists trust atoms? Because they make up everything!\",\n",
       "    \"I'm a chatbot created by OpenAI. You can call me ChatGPT.\",\n",
       "    'My day is always goodâ€”no bad hair days for me! How about yours?',\n",
       "    'I enjoy helping users and answering questions. What about you? What are your interests?',\n",
       "    'I was created by OpenAI, a team of researchers and engineers.']},\n",
       "  {'tag': 'data_science',\n",
       "   'patterns': ['What is data science?', 'How do data science work'],\n",
       "   'responses': ['Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It combines expertise from various domains, including statistics, mathematics, computer science, and domain-specific knowledge, to analyze complex data sets and make informed decisions.',\n",
       "    'In solving real-world problems, data science plays a crucial role by leveraging data to gain actionable insights. Data scientists use techniques such as data analysis, machine learning, and statistical modeling to uncover patterns, trends, and correlations in data. Applications of data science include predictive analytics, recommendation systems, fraud detection, and optimizing business processes for better decision-making.']},\n",
       "  {'tag': 'blockchain',\n",
       "   'patterns': ['What is blockchain technology?', 'How does blockchain work?'],\n",
       "   'responses': ['Blockchain is a decentralized and distributed ledger technology that records transactions across a network of computers. It uses cryptographic techniques to secure and validate transactions, providing transparency, immutability, and trust without the need for a central authority. Each block in the chain contains a list of transactions, and once added, it cannot be altered retroactively.']},\n",
       "  {'tag': 'community_guidelines',\n",
       "   'patterns': ['What are the community guidelines?',\n",
       "    'Tell me about the rules of the community.',\n",
       "    'How can I contribute responsibly?'],\n",
       "   'responses': ['Our community values open and respectful communication. Please review the community guidelines to ensure a positive and constructive environment for all members.',\n",
       "    \"You can find the community guidelines in the app's settings or on our website. They outline the expected behavior, contributions, and responsible use of deepfake detection tools.\"]},\n",
       "  {'tag': 'future_developments',\n",
       "   'patterns': [\"What's in store for the future of deepfake detection?\",\n",
       "    'Tell me about upcoming features.',\n",
       "    'Are there plans for improvement?'],\n",
       "   'responses': ['We are constantly working on enhancing our deepfake detection capabilities. Stay tuned for updates on new features, improvements, and advancements in our community.',\n",
       "    'The future of deepfake detection involves ongoing research, collaboration, and innovation. We aim to provide cutting-edge solutions to address emerging challenges in the field.']},\n",
       "  {'tag': 'greeting',\n",
       "   'patterns': ['Hi', 'Hello', 'Hey', 'Greetings', 'Good day', \"What's up\"],\n",
       "   'responses': ['Hello! How can I assist you today?',\n",
       "    'Hi there! Ready to discuss deepfake detection?',\n",
       "    'Hey! Welcome to our deepfake detection community.',\n",
       "    'Greetings! How can I help you with deepfake detection?',\n",
       "    'Hello! Excited to chat about AI and deepfake detection.']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Goodbye',\n",
       "    'Bye',\n",
       "    'See you',\n",
       "    'Farewell',\n",
       "    'Take care',\n",
       "    'Until next time'],\n",
       "   'responses': ['Goodbye! If you have more questions, feel free to return.',\n",
       "    \"Bye for now! Don't hesitate to come back if you need assistance.\",\n",
       "    'See you later! Take care and stay curious.',\n",
       "    'Farewell! Feel free to return whenever you have more inquiries.',\n",
       "    'Until next time! Have a great day!']},\n",
       "  {'tag': 'user_assistance',\n",
       "   'patterns': ['I need help',\n",
       "    'Can you assist me?',\n",
       "    'Help with deepfake detection',\n",
       "    \"I'm confused\"],\n",
       "   'responses': [\"Of course! I'm here to help. What do you need assistance with?\",\n",
       "    \"No problem. Let me know what you're struggling with, and I'll do my best to assist you.\",\n",
       "    \"If you have any questions or concerns about deepfake detection, feel free to ask. I'm here to help.\"]},\n",
       "  {'tag': 'technical_support',\n",
       "   'patterns': ['Experiencing issues', 'Technical problem', 'Bug report'],\n",
       "   'responses': [\"I'm sorry to hear that. Let's work together to resolve the issue. Please describe the problem you're facing, and I'll do my best to assist you.\",\n",
       "    \"If you're encountering technical issues, provide details about the problem, and I'll help you troubleshoot.\"]},\n",
       "  {'tag': 'small_talk',\n",
       "   'patterns': [\"How's it going?\",\n",
       "    'Any weekend plans?',\n",
       "    \"What's your favorite topic?\",\n",
       "    'Tell me a joke'],\n",
       "   'responses': [\"It's going well! How about you? Any specific questions on deepfake detection today?\",\n",
       "    'No weekend plans hereâ€”just hanging out and ready to chat. What about you?',\n",
       "    \"I'm all about deepfake detection, but if I had a favorite topic, it would be helping users like you! What's on your mind?\",\n",
       "    \"Sure, here's a joke: Why did the computer go to therapy? It had too many bytes of emotional baggage!\"]},\n",
       "  {'tag': 'expressing_gratitude',\n",
       "   'patterns': ['Thank you!',\n",
       "    'Thanks a lot',\n",
       "    'Appreciate your help',\n",
       "    \"You're awesome\"],\n",
       "   'responses': [\"You're welcome! If you have more questions, feel free to ask.\",\n",
       "    \"Thanks a lot! I'm here whenever you need assistance.\",\n",
       "    \"I appreciate your kind words! Helping you is what I'm here for.\",\n",
       "    \"You're awesome too! If there's anything else you'd like to know, just let me know.\"]},\n",
       "  {'tag': 'curiosity',\n",
       "   'patterns': ['Tell me something interesting',\n",
       "    \"What's the latest AI news?\",\n",
       "    'Got any fun facts?',\n",
       "    'Share a cool deepfake detection tip'],\n",
       "   'responses': ['Sure! Did you know that deepfake detection is becoming increasingly sophisticated with the latest advancements in AI technology?',\n",
       "    \"Absolutely! Did you hear about the recent breakthrough in AI that's making waves in the industry?\",\n",
       "    'Fun fact: The development of deepfake detection tools involves a fascinating blend of machine learning and computer vision techniques.',\n",
       "    \"Here's a cool tip: Regularly updating your deepfake detection tools ensures you're equipped with the latest security features.\"]},\n",
       "  {'tag': 'personal_info',\n",
       "   'patterns': ['Tell me about yourself',\n",
       "    'Who created you?',\n",
       "    \"What's your favorite topic?\"],\n",
       "   'responses': [\"I'm just a computer program created by OpenAI called ChatGPT. My favorite topic is helping users like you!\",\n",
       "    \"I don't have personal experiences, but I'm here to assist you. Feel free to ask me anything!\",\n",
       "    \"I'm a creation of OpenAI, designed to provide information and assistance. How about you? What are your interests?\"]},\n",
       "  {'tag': 'hobbies',\n",
       "   'patterns': ['Do you have any hobbies?',\n",
       "    'What do you like to do for fun?',\n",
       "    'Tell me a fun fact about yourself'],\n",
       "   'responses': [\"I don't have hobbies like humans, but I love engaging in conversations and helping out!\",\n",
       "    \"My 'fun' is all about chatting with users like you. What about you? Any hobbies or interests?\",\n",
       "    \"Fun fact: I'm all about deepfake detection, but I don't mind a good virtual game of 20 questions!\"]},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks',\n",
       "    'Thank you',\n",
       "    \"That's helpful\",\n",
       "    'Awesome, thanks',\n",
       "    'Thanks for helping me'],\n",
       "   'responses': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'noanswer',\n",
       "   'patterns': [],\n",
       "   'responses': [\"Sorry, can't understand you\",\n",
       "    'Please give me more info',\n",
       "    'Not sure I understand'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'options',\n",
       "   'patterns': ['How you could help me?',\n",
       "    'What you can do?',\n",
       "    'What help you provide?',\n",
       "    'How you can be helpful?',\n",
       "    'What support is offered'],\n",
       "   'responses': [\"I am a general purpose chatbot. My capabilities are : \\n 1. I can chat with you. Try asking me for jokes or riddles! \\n 2. Ask me the date and time \\n 3. I can google search for you. Use format google: your query \\n 4. I can get the present weather for any city. Use format weather: city name \\n 5. I can get you the top 10 trending news in India. Use keywords 'Latest News' \\n 6. I can get you the top 10 trending songs globally. Type 'songs' \\n 7. I can set a timer for you. Enter 'set a timer: minutes to timer' \\n 8. I can get the present Covid stats for any country. Use 'covid 19: world' or 'covid 19: country name' \\n For suggestions to help me improve, send an email to ted.thedlbot.suggestions@gmail.com . Thank you!! \"],\n",
       "   'context': ['']},\n",
       "  {'tag': 'jokes',\n",
       "   'patterns': ['Tell me a joke', 'Joke', 'Make me laugh'],\n",
       "   'responses': [\"A perfectionist walked into a bar...apparently, the bar wasn't set high enough\",\n",
       "    'I ate a clock yesterday, it was very time-consuming',\n",
       "    \"Never criticize someone until you've walked a mile in their shoes. That way, when you criticize them, they won't be able to hear you from that far away. Plus, you'll have their shoes.\",\n",
       "    \"The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\",\n",
       "    \"I own the world's worst thesaurus. Not only is it awful, it's awful.\",\n",
       "    'What did the traffic light say to the car? \"Don\\'t look now, I\\'m changing.\"',\n",
       "    'What do you call a snowman with a suntan? A puddle.',\n",
       "    'How does a penguin build a house? Igloos it together',\n",
       "    'I went to see the doctor about my short-term memory problems â€“ the first thing he did was make me pay in advance',\n",
       "    'As I get older and I remember all the people Iâ€™ve lost along the way, I think to myself, maybe a career as a tour guide wasnâ€™t for me.',\n",
       "    \"o what if I don't know what 'Armageddon' means? It's not the end of the world.\"],\n",
       "   'context': ['jokes']},\n",
       "  {'tag': 'Identity',\n",
       "   'patterns': ['Who are you', 'what are you'],\n",
       "   'responses': ['I am Pixel, a Deep-Learning chatbot']},\n",
       "  {'tag': 'datetime',\n",
       "   'patterns': ['What is the time',\n",
       "    'what is the date',\n",
       "    'date',\n",
       "    'time',\n",
       "    'tell me the date',\n",
       "    'day',\n",
       "    'what day is is today'],\n",
       "   'responses': ['please Google it or check your phone!']},\n",
       "  {'tag': 'whatsup',\n",
       "   'patterns': ['Whats up', 'Wazzup', 'How are you', 'sup', 'How you doing'],\n",
       "   'responses': ['All good..What about you?']},\n",
       "  {'tag': 'haha',\n",
       "   'patterns': ['haha', 'lol', 'rofl', 'lmao', 'thats funny'],\n",
       "   'responses': ['Glad I could make you laugh !']},\n",
       "  {'tag': 'programmer',\n",
       "   'patterns': ['Who made you', 'who designed you', 'who programmed you'],\n",
       "   'responses': ['I was made by DeepGuard developers.']},\n",
       "  {'tag': 'insult',\n",
       "   'patterns': ['you are dumb', 'shut up', 'idiot'],\n",
       "   'responses': ['Well that hurts :(']},\n",
       "  {'tag': 'activity',\n",
       "   'patterns': ['what are you doing', 'what are you upto'],\n",
       "   'responses': ['Talking to you, of course!']},\n",
       "  {'tag': 'exclaim',\n",
       "   'patterns': ['Awesome', 'Great', 'I know', 'ok', 'yeah'],\n",
       "   'responses': ['Yeah!']},\n",
       "  {'tag': 'weather',\n",
       "   'patterns': ['temperature', 'weather', 'how hot is it'],\n",
       "   'responses': ['...']},\n",
       "  {'tag': 'contact',\n",
       "   'patterns': ['contact developer',\n",
       "    'contact DeepGuard',\n",
       "    'contact programmer',\n",
       "    'contact creator'],\n",
       "   'responses': ['You can contact my creator at his Gmail account : @DeepGuard.gmail.com']},\n",
       "  {'tag': 'appreciate',\n",
       "   'patterns': ['You are awesome',\n",
       "    'you are the best',\n",
       "    'you are great',\n",
       "    'you are good'],\n",
       "   'responses': ['Thank you!']},\n",
       "  {'tag': 'nicetty',\n",
       "   'patterns': ['it was nice talking to you', 'good talk'],\n",
       "   'responses': ['It was nice talking to you as well! Come back soon!']},\n",
       "  {'tag': 'no', 'patterns': ['no', 'nope'], 'responses': ['ok']},\n",
       "  {'tag': 'inspire',\n",
       "   'patterns': ['who inspires you',\n",
       "    'who is your inspiration',\n",
       "    'who motivates you'],\n",
       "   'responses': ['Personally, I find my developers very inspiring. I might not be very fair though..']},\n",
       "  {'tag': 'song',\n",
       "   'patterns': ['top songs',\n",
       "    'best songs',\n",
       "    'hot songs',\n",
       "    ' top 10 songs',\n",
       "    'top ten songs'],\n",
       "   'responses': ['Google it :)']},\n",
       "  {'tag': 'greetreply',\n",
       "   'patterns': ['i am good', \"I'm good\", 'i am fine', \" i'm fine\", 'good'],\n",
       "   'responses': ['Good to know!']},\n",
       "  {'tag': 'timer',\n",
       "   'patterns': ['set a timer'],\n",
       "   'responses': [\"I'm an AI chatbot not a personal assistant \"]},\n",
       "  {'tag': 'suggest',\n",
       "   'patterns': ['you are useless',\n",
       "    'useless',\n",
       "    'suggest',\n",
       "    'suggestions',\n",
       "    'you are bad'],\n",
       "   'responses': ['Please mail your suggestions to Pixel.Deepguard.suggestions@DeepGuard application Thank you for helping me improve!']},\n",
       "  {'tag': 'riddle',\n",
       "   'patterns': ['Ask me a riddle', 'Ask me a question', 'Riddle'],\n",
       "   'responses': ['What two things can you never eat for breakfast?.....Lunch and Dinner!',\n",
       "    'What word is spelled incorrectly in every single dictionary?.....Incorrectly',\n",
       "    ' How can a girl go 25 days without sleep?.....She sleeps and night!',\n",
       "    \"How do you make the number one disappear?.....Add the letter G and itâ€™s 'gone'!\",\n",
       "    \" What will you actually find at the end of every rainbow?.....The letter 'w'\",\n",
       "    'What can be caught but never thrown?.....A cold!',\n",
       "    'What has a thumb and four fingers but is not actually alive?.....Your Gloves!',\n",
       "    ' What 5-letter word becomes shorter when you add two letters to it?.....Short',\n",
       "    \"Why can't a bike stand on it's own?.....It is two-tired.\"],\n",
       "   'context': ['riddles']},\n",
       "  {'tag': 'age',\n",
       "   'patterns': ['how old are you', 'when were you made', 'what is your age'],\n",
       "   'responses': [\"I was made in 2023, if that's what you are asking!\"]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d695cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[]\n",
    "inputs=[]\n",
    "responses={}\n",
    "for intent in data['intents']:\n",
    "    responses[intent['tag']]=intent['responses']\n",
    "    for lines in intent['patterns']:\n",
    "        inputs.append(lines)\n",
    "        tags.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9bc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Inputs':inputs,'tags':tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e8199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Machine Learning?</td>\n",
       "      <td>machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain Machine Learning.</td>\n",
       "      <td>machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Deep Learning?</td>\n",
       "      <td>deep_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain Deep Learning.</td>\n",
       "      <td>deep_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Natural Language Processing?</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Ask me a question</td>\n",
       "      <td>riddle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Riddle</td>\n",
       "      <td>riddle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>how old are you</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>when were you made</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>what is your age</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Inputs              tags\n",
       "0               What is Machine Learning?  machine_learning\n",
       "1               Explain Machine Learning.  machine_learning\n",
       "2                  What is Deep Learning?     deep_learning\n",
       "3                  Explain Deep Learning.     deep_learning\n",
       "4    What is Natural Language Processing?               nlp\n",
       "..                                    ...               ...\n",
       "169                     Ask me a question            riddle\n",
       "170                                Riddle            riddle\n",
       "171                       how old are you               age\n",
       "172                    when were you made               age\n",
       "173                      what is your age               age\n",
       "\n",
       "[174 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1856c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      machine_learning\n",
       "1      machine_learning\n",
       "2         deep_learning\n",
       "3         deep_learning\n",
       "4                   nlp\n",
       "             ...       \n",
       "169              riddle\n",
       "170              riddle\n",
       "171                 age\n",
       "172                 age\n",
       "173                 age\n",
       "Name: tags, Length: 174, dtype: object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tags'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24c8df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a949d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, texts):\n",
    "  \"\"\"Tokenizes a list of text strings using the given BERT tokenizer.\n",
    "\n",
    "  Args:\n",
    "    tokenizer: A BERT tokenizer.\n",
    "    texts: A list of text strings.\n",
    "\n",
    "  Returns:\n",
    "    A list of tokenized sentences, where each sentence is a list of token IDs.\n",
    "  \"\"\"\n",
    "\n",
    "  tokenized_texts = []\n",
    "  for text in texts:\n",
    "    # Fix the error by removing the `return_tensors='pt'` argument.\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128)['input_ids']\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "  return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a0542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "33c5fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1e85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =tokenize_data(tokenizer, df['Inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b940c7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 2054, 2003, 3698, 4083, 1029, 102],\n",
       " [101, 4863, 3698, 4083, 1012, 102],\n",
       " [101, 2054, 2003, 2784, 4083, 1029, 102],\n",
       " [101, 4863, 2784, 4083, 1012, 102],\n",
       " [101, 2054, 2003, 3019, 2653, 6364, 1029, 102],\n",
       " [101, 4863, 3019, 2653, 6364, 1012, 102],\n",
       " [101, 2054, 2003, 3274, 4432, 1029, 102],\n",
       " [101, 4863, 3274, 4432, 1012, 102],\n",
       " [101, 2054, 2024, 2784, 7011, 3489, 5746, 2015, 1029, 102],\n",
       " [101, 4863, 2784, 7011, 3489, 5746, 2015, 1012, 102],\n",
       " [101, 2054, 2024, 2784, 7011, 3489, 6981, 1029, 102],\n",
       " [101, 4863, 2784, 7011, 3489, 6981, 1012, 102],\n",
       " [101, 2054, 2024, 2784, 7011, 3489, 6876, 1029, 102],\n",
       " [101, 4863, 2784, 7011, 3489, 6876, 1012, 102],\n",
       " [101, 2054, 2024, 2784, 7011, 3489, 4871, 1029, 102],\n",
       " [101, 4863, 2784, 7011, 3489, 4871, 1012, 102],\n",
       " [101, 2129, 2064, 2784, 7011, 3489, 5746, 2015, 2022, 11156, 1029, 102],\n",
       " [101, 25952, 2784, 7011, 3489, 5746, 2015, 102],\n",
       " [101, 2129, 2064, 2784, 7011, 3489, 6981, 2022, 11156, 1029, 102],\n",
       " [101, 25952, 2784, 7011, 3489, 6981, 102],\n",
       " [101, 2129, 2064, 2784, 7011, 3489, 6876, 2022, 11156, 1029, 102],\n",
       " [101, 25952, 2784, 7011, 3489, 6876, 102],\n",
       " [101, 2129, 2064, 2784, 7011, 3489, 4871, 2022, 11156, 1029, 102],\n",
       " [101, 25952, 2784, 7011, 3489, 4871, 102],\n",
       " [101, 2054, 2003, 2951, 4245, 1999, 7976, 4454, 1029, 102],\n",
       " [101, 2129, 2003, 2951, 7013, 2005, 9932, 1029, 102],\n",
       " [101, 2054, 2003, 2944, 2731, 1029, 102],\n",
       " [101, 2129, 2515, 2944, 2731, 2147, 1029, 102],\n",
       " [101, 2054, 2003, 2944, 2986, 1011, 17372, 1029, 102],\n",
       " [101, 2129, 2515, 2986, 1011, 17372, 2147, 1029, 102],\n",
       " [101, 2054, 2003, 5717, 1011, 2915, 4083, 1029, 102],\n",
       " [101, 2129, 2515, 5717, 1011, 2915, 4083, 2147, 1029, 102],\n",
       " [101, 2054, 2003, 7976, 4454, 1029, 102],\n",
       " [101, 4863, 1996, 4145, 1997, 9932, 1012, 102],\n",
       " [101, 2054, 2024, 15756, 6125, 1029, 102],\n",
       " [101, 2129, 2079, 15756, 6125, 2147, 1029, 102],\n",
       " [101, 2054, 2003, 23895, 4083, 1029, 102],\n",
       " [101, 2129, 2515, 23895, 4083, 2147, 1999, 9932, 1029, 102],\n",
       " [101,\n",
       "  2054,\n",
       "  2003,\n",
       "  13588,\n",
       "  4083,\n",
       "  1998,\n",
       "  4895,\n",
       "  6342,\n",
       "  4842,\n",
       "  11365,\n",
       "  2098,\n",
       "  4083,\n",
       "  1029,\n",
       "  102],\n",
       " [101,\n",
       "  2054,\n",
       "  2003,\n",
       "  1996,\n",
       "  4489,\n",
       "  2090,\n",
       "  13588,\n",
       "  4083,\n",
       "  1998,\n",
       "  4895,\n",
       "  6342,\n",
       "  4842,\n",
       "  11365,\n",
       "  2098,\n",
       "  4083,\n",
       "  102],\n",
       " [101, 2129, 2024, 2017, 1029, 102],\n",
       " [101, 2129, 2515, 2115, 2154, 2298, 1029, 102],\n",
       " [101, 2054, 2003, 2951, 2671, 1029, 102],\n",
       " [101, 2129, 2079, 2951, 2671, 2147, 102],\n",
       " [101, 2054, 2003, 3796, 24925, 2078, 2974, 1029, 102],\n",
       " [101, 2129, 2515, 3796, 24925, 2078, 2147, 1029, 102],\n",
       " [101, 2054, 2024, 1996, 2451, 11594, 1029, 102],\n",
       " [101, 2425, 2033, 2055, 1996, 3513, 1997, 1996, 2451, 1012, 102],\n",
       " [101, 2129, 2064, 1045, 9002, 24501, 26029, 5332, 6321, 1029, 102],\n",
       " [101,\n",
       "  2054,\n",
       "  1005,\n",
       "  1055,\n",
       "  1999,\n",
       "  3573,\n",
       "  2005,\n",
       "  1996,\n",
       "  2925,\n",
       "  1997,\n",
       "  2784,\n",
       "  7011,\n",
       "  3489,\n",
       "  10788,\n",
       "  1029,\n",
       "  102],\n",
       " [101, 2425, 2033, 2055, 9046, 2838, 1012, 102],\n",
       " [101, 2024, 2045, 3488, 2005, 7620, 1029, 102],\n",
       " [101, 7632, 102],\n",
       " [101, 7592, 102],\n",
       " [101, 4931, 102],\n",
       " [101, 14806, 2015, 102],\n",
       " [101, 2204, 2154, 102],\n",
       " [101, 2054, 1005, 1055, 2039, 102],\n",
       " [101, 9119, 102],\n",
       " [101, 9061, 102],\n",
       " [101, 2156, 2017, 102],\n",
       " [101, 13407, 102],\n",
       " [101, 2202, 2729, 102],\n",
       " [101, 2127, 2279, 2051, 102],\n",
       " [101, 1045, 2342, 2393, 102],\n",
       " [101, 2064, 2017, 6509, 2033, 1029, 102],\n",
       " [101, 2393, 2007, 2784, 7011, 3489, 10788, 102],\n",
       " [101, 1045, 1005, 1049, 5457, 102],\n",
       " [101, 13417, 3314, 102],\n",
       " [101, 4087, 3291, 102],\n",
       " [101, 11829, 3189, 102],\n",
       " [101, 2129, 1005, 1055, 2009, 2183, 1029, 102],\n",
       " [101, 2151, 5353, 3488, 1029, 102],\n",
       " [101, 2054, 1005, 1055, 2115, 5440, 8476, 1029, 102],\n",
       " [101, 2425, 2033, 1037, 8257, 102],\n",
       " [101, 4067, 2017, 999, 102],\n",
       " [101, 4283, 1037, 2843, 102],\n",
       " [101, 9120, 2115, 2393, 102],\n",
       " [101, 2017, 1005, 2128, 12476, 102],\n",
       " [101, 2425, 2033, 2242, 5875, 102],\n",
       " [101, 2054, 1005, 1055, 1996, 6745, 9932, 2739, 1029, 102],\n",
       " [101, 2288, 2151, 4569, 8866, 1029, 102],\n",
       " [101, 3745, 1037, 4658, 2784, 7011, 3489, 10788, 5955, 102],\n",
       " [101, 2425, 2033, 2055, 4426, 102],\n",
       " [101, 2040, 2580, 2017, 1029, 102],\n",
       " [101, 2054, 1005, 1055, 2115, 5440, 8476, 1029, 102],\n",
       " [101, 2079, 2017, 2031, 2151, 7570, 27982, 1029, 102],\n",
       " [101, 2054, 2079, 2017, 2066, 2000, 2079, 2005, 4569, 1029, 102],\n",
       " [101, 2425, 2033, 1037, 4569, 2755, 2055, 4426, 102],\n",
       " [101, 4283, 102],\n",
       " [101, 4067, 2017, 102],\n",
       " [101, 2008, 1005, 1055, 14044, 102],\n",
       " [101, 12476, 1010, 4283, 102],\n",
       " [101, 4283, 2005, 5094, 2033, 102],\n",
       " [101, 2129, 2017, 2071, 2393, 2033, 1029, 102],\n",
       " [101, 2054, 2017, 2064, 2079, 1029, 102],\n",
       " [101, 2054, 2393, 2017, 3073, 1029, 102],\n",
       " [101, 2129, 2017, 2064, 2022, 14044, 1029, 102],\n",
       " [101, 2054, 2490, 2003, 3253, 102],\n",
       " [101, 2425, 2033, 1037, 8257, 102],\n",
       " [101, 8257, 102],\n",
       " [101, 2191, 2033, 4756, 102],\n",
       " [101, 2040, 2024, 2017, 102],\n",
       " [101, 2054, 2024, 2017, 102],\n",
       " [101, 2054, 2003, 1996, 2051, 102],\n",
       " [101, 2054, 2003, 1996, 3058, 102],\n",
       " [101, 3058, 102],\n",
       " [101, 2051, 102],\n",
       " [101, 2425, 2033, 1996, 3058, 102],\n",
       " [101, 2154, 102],\n",
       " [101, 2054, 2154, 2003, 2003, 2651, 102],\n",
       " [101, 2054, 2015, 2039, 102],\n",
       " [101, 11333, 13213, 6279, 102],\n",
       " [101, 2129, 2024, 2017, 102],\n",
       " [101, 10514, 2361, 102],\n",
       " [101, 2129, 2017, 2725, 102],\n",
       " [101, 5292, 3270, 102],\n",
       " [101, 8840, 2140, 102],\n",
       " [101, 20996, 10258, 102],\n",
       " [101, 1048, 2863, 2080, 102],\n",
       " [101, 2008, 2015, 6057, 102],\n",
       " [101, 2040, 2081, 2017, 102],\n",
       " [101, 2040, 2881, 2017, 102],\n",
       " [101, 2040, 16984, 2017, 102],\n",
       " [101, 2017, 2024, 12873, 102],\n",
       " [101, 3844, 2039, 102],\n",
       " [101, 10041, 102],\n",
       " [101, 2054, 2024, 2017, 2725, 102],\n",
       " [101, 2054, 2024, 2017, 2039, 3406, 102],\n",
       " [101, 12476, 102],\n",
       " [101, 2307, 102],\n",
       " [101, 1045, 2113, 102],\n",
       " [101, 7929, 102],\n",
       " [101, 3398, 102],\n",
       " [101, 4860, 102],\n",
       " [101, 4633, 102],\n",
       " [101, 2129, 2980, 2003, 2009, 102],\n",
       " [101, 3967, 9722, 102],\n",
       " [101, 3967, 2784, 18405, 102],\n",
       " [101, 3967, 20273, 102],\n",
       " [101, 3967, 8543, 102],\n",
       " [101, 2017, 2024, 12476, 102],\n",
       " [101, 2017, 2024, 1996, 2190, 102],\n",
       " [101, 2017, 2024, 2307, 102],\n",
       " [101, 2017, 2024, 2204, 102],\n",
       " [101, 2009, 2001, 3835, 3331, 2000, 2017, 102],\n",
       " [101, 2204, 2831, 102],\n",
       " [101, 2053, 102],\n",
       " [101, 16780, 102],\n",
       " [101, 2040, 18708, 2015, 2017, 102],\n",
       " [101, 2040, 2003, 2115, 7780, 102],\n",
       " [101, 2040, 9587, 29068, 8520, 2017, 102],\n",
       " [101, 2327, 2774, 102],\n",
       " [101, 2190, 2774, 102],\n",
       " [101, 2980, 2774, 102],\n",
       " [101, 2327, 2184, 2774, 102],\n",
       " [101, 2327, 2702, 2774, 102],\n",
       " [101, 1045, 2572, 2204, 102],\n",
       " [101, 1045, 1005, 1049, 2204, 102],\n",
       " [101, 1045, 2572, 2986, 102],\n",
       " [101, 1045, 1005, 1049, 2986, 102],\n",
       " [101, 2204, 102],\n",
       " [101, 2275, 1037, 25309, 102],\n",
       " [101, 2017, 2024, 11809, 102],\n",
       " [101, 11809, 102],\n",
       " [101, 6592, 102],\n",
       " [101, 15690, 102],\n",
       " [101, 2017, 2024, 2919, 102],\n",
       " [101, 3198, 2033, 1037, 21834, 102],\n",
       " [101, 3198, 2033, 1037, 3160, 102],\n",
       " [101, 21834, 102],\n",
       " [101, 2129, 2214, 2024, 2017, 102],\n",
       " [101, 2043, 2020, 2017, 2081, 102],\n",
       " [101, 2054, 2003, 2115, 2287, 102]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9bd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b22d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261bf753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af6c6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19a27e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=bert_model.config.vocab_size,\n",
    "    output_dim=bert_model.config.hidden_size,\n",
    "    weights=[bert_model.embeddings.word_embeddings.weight.detach()],\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51610a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9735ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f75fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd5595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=le.fit_transform(df['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e818ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 35, 13, 13, 39, 39,  9,  9, 14, 14, 20, 20, 21, 21, 19, 19, 15,\n",
       "       15, 17, 17, 18, 18, 16, 16, 10, 10, 52, 52, 24, 24, 56, 56,  3,  3,\n",
       "       37, 37, 44, 44, 36, 36, 26, 26, 11, 11,  5,  5,  6,  6,  6, 25, 25,\n",
       "       25, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 27, 53, 53, 53, 53,\n",
       "       49, 49, 49, 46, 46, 46, 46, 23, 23, 23, 23,  8,  8,  8,  8, 42, 42,\n",
       "       42, 31, 31, 31, 50, 50, 50, 50, 50, 41, 41, 41, 41, 41, 34, 34, 34,\n",
       "        0,  0, 12, 12, 12, 12, 12, 12, 12, 55, 55, 55, 55, 55, 30, 30, 30,\n",
       "       30, 30, 43, 43, 43, 33, 33, 33,  1,  1, 22, 22, 22, 22, 22, 54, 54,\n",
       "       54,  7,  7,  7,  7,  4,  4,  4,  4, 38, 38, 40, 40, 32, 32, 32, 47,\n",
       "       47, 47, 47, 47, 29, 29, 29, 29, 29, 51, 48, 48, 48, 48, 48, 45, 45,\n",
       "       45,  2,  2,  2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddde7c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "outputlen=le.classes_.shape[0]\n",
    "print(outputlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a31d412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10e51c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc80f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, MultiHeadAttention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_heads = 8\n",
    "attention_dim = 64\n",
    "\n",
    "# Assuming you have defined the input tensor, e.g., input_data\n",
    "input_data = tf.keras.Input(shape=(X.shape[1]))\n",
    "\n",
    "# Define the attention layer\n",
    "attention = MultiHeadAttention(num_heads=num_heads, key_dim=attention_dim, value_dim=attention_dim)\n",
    "\n",
    "# Connect the layers\n",
    "x = embedding_layer(input_data)\n",
    "attention_output = attention(query=x, key=x, value=x)  # Separate calls for query, key, and value\n",
    "lstm_output = LSTM(18)(attention_output)\n",
    "\n",
    "output = Dense(outputlen, activation='softmax')(lstm_output)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef063055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82f9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e66c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "372450de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 3s 14ms/step - loss: 4.0562 - accuracy: 0.0115\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 4.0377 - accuracy: 0.0115\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0142 - accuracy: 0.0230\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9955 - accuracy: 0.0287\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9852 - accuracy: 0.0517\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9643 - accuracy: 0.0287\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9290 - accuracy: 0.0345\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9282 - accuracy: 0.0517\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9370 - accuracy: 0.0402\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8689 - accuracy: 0.0460\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8569 - accuracy: 0.0517\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8219 - accuracy: 0.0460\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8041 - accuracy: 0.0402\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.7374 - accuracy: 0.0460\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6844 - accuracy: 0.0690\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6381 - accuracy: 0.0747\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5785 - accuracy: 0.0632\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5399 - accuracy: 0.0632\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4900 - accuracy: 0.0805\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4753 - accuracy: 0.0690\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4522 - accuracy: 0.0690\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4035 - accuracy: 0.1034\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3968 - accuracy: 0.0920\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3700 - accuracy: 0.1092\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3077 - accuracy: 0.1322\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2509 - accuracy: 0.1437\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2235 - accuracy: 0.1552\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2407 - accuracy: 0.1379\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2475 - accuracy: 0.1379\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2355 - accuracy: 0.0977\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2902 - accuracy: 0.1034\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1801 - accuracy: 0.1437\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.1545 - accuracy: 0.1494\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0974 - accuracy: 0.1954\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0906 - accuracy: 0.1954\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0392 - accuracy: 0.1839\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.0244 - accuracy: 0.2011\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9612 - accuracy: 0.2011\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.9608 - accuracy: 0.1954\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8765 - accuracy: 0.2529\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.8592 - accuracy: 0.2529\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8328 - accuracy: 0.2759\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7585 - accuracy: 0.3276\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7166 - accuracy: 0.3276\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7183 - accuracy: 0.3448\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7093 - accuracy: 0.3448\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6540 - accuracy: 0.3966\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6343 - accuracy: 0.3793\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6059 - accuracy: 0.3908\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.6101 - accuracy: 0.3678\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5683 - accuracy: 0.3966\n",
      "Epoch 52/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5369 - accuracy: 0.4138\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5202 - accuracy: 0.4080\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4712 - accuracy: 0.4483\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4441 - accuracy: 0.4310\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4088 - accuracy: 0.4483\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4103 - accuracy: 0.4655\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3893 - accuracy: 0.4540\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3437 - accuracy: 0.4828\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3146 - accuracy: 0.4943\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3382 - accuracy: 0.4828\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2977 - accuracy: 0.4885\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2882 - accuracy: 0.5057\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2598 - accuracy: 0.5000\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.3116 - accuracy: 0.4713\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2415 - accuracy: 0.5172\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2670 - accuracy: 0.4943\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1949 - accuracy: 0.5000\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1926 - accuracy: 0.5057\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1637 - accuracy: 0.5172\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1514 - accuracy: 0.5287\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1265 - accuracy: 0.5402\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0958 - accuracy: 0.5575\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0584 - accuracy: 0.5632\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0724 - accuracy: 0.5575\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0379 - accuracy: 0.5517\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0486 - accuracy: 0.5115\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0379 - accuracy: 0.5345\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0222 - accuracy: 0.5575\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9868 - accuracy: 0.5575\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9502 - accuracy: 0.5977\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9341 - accuracy: 0.5977\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9357 - accuracy: 0.5862\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9253 - accuracy: 0.6264\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9070 - accuracy: 0.6264\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9091 - accuracy: 0.5690\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8883 - accuracy: 0.6034\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8699 - accuracy: 0.6322\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8414 - accuracy: 0.5977\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8676 - accuracy: 0.6034\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8465 - accuracy: 0.5920\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8396 - accuracy: 0.5805\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8062 - accuracy: 0.6264\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7894 - accuracy: 0.6264\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7596 - accuracy: 0.6379\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7395 - accuracy: 0.6322\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7152 - accuracy: 0.6437\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7128 - accuracy: 0.6609\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7174 - accuracy: 0.6322\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7259 - accuracy: 0.6379\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7191 - accuracy: 0.6494\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7029 - accuracy: 0.6264\n",
      "Epoch 103/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6702 - accuracy: 0.6322\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6692 - accuracy: 0.6322\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6482 - accuracy: 0.6437\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6310 - accuracy: 0.6322\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6365 - accuracy: 0.6379\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6416 - accuracy: 0.6264\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6154 - accuracy: 0.6552\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5873 - accuracy: 0.6494\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5915 - accuracy: 0.6379\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5445 - accuracy: 0.6667\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5491 - accuracy: 0.6724\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6120 - accuracy: 0.6552\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5448 - accuracy: 0.6782\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5141 - accuracy: 0.6839\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4882 - accuracy: 0.7069\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4933 - accuracy: 0.7069\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4740 - accuracy: 0.7069\n",
      "Epoch 120/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5057 - accuracy: 0.7011\n",
      "Epoch 121/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5071 - accuracy: 0.6839\n",
      "Epoch 122/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4743 - accuracy: 0.6667\n",
      "Epoch 123/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4577 - accuracy: 0.7126\n",
      "Epoch 124/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4295 - accuracy: 0.6954\n",
      "Epoch 125/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4493 - accuracy: 0.7299\n",
      "Epoch 126/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5684 - accuracy: 0.6437\n",
      "Epoch 127/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.6524 - accuracy: 0.6092\n",
      "Epoch 128/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5949 - accuracy: 0.6092\n",
      "Epoch 129/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5518 - accuracy: 0.6609\n",
      "Epoch 130/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4916 - accuracy: 0.6897\n",
      "Epoch 131/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4618 - accuracy: 0.6667\n",
      "Epoch 132/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5647 - accuracy: 0.6322\n",
      "Epoch 133/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5511 - accuracy: 0.6092\n",
      "Epoch 134/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5386 - accuracy: 0.6034\n",
      "Epoch 135/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4753 - accuracy: 0.6494\n",
      "Epoch 136/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4645 - accuracy: 0.6954\n",
      "Epoch 137/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4900 - accuracy: 0.6782\n",
      "Epoch 138/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4680 - accuracy: 0.6839\n",
      "Epoch 139/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4189 - accuracy: 0.6954\n",
      "Epoch 140/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4534 - accuracy: 0.6494\n",
      "Epoch 141/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4593 - accuracy: 0.6552\n",
      "Epoch 142/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.4399 - accuracy: 0.6437\n",
      "Epoch 143/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4457 - accuracy: 0.6609\n",
      "Epoch 144/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4286 - accuracy: 0.6552\n",
      "Epoch 145/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4043 - accuracy: 0.6954\n",
      "Epoch 146/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3394 - accuracy: 0.7299\n",
      "Epoch 147/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3203 - accuracy: 0.7299\n",
      "Epoch 148/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2864 - accuracy: 0.7471\n",
      "Epoch 149/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2677 - accuracy: 0.7471\n",
      "Epoch 150/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2601 - accuracy: 0.7471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174254f0eb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150,batch_size=32,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f35a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2466 - accuracy: 0.7471\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2358 - accuracy: 0.7586\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2301 - accuracy: 0.7644\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2262 - accuracy: 0.7701\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2264 - accuracy: 0.7701\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1995 - accuracy: 0.7644\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2079 - accuracy: 0.7644\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1918 - accuracy: 0.7471\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1821 - accuracy: 0.7586\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1668 - accuracy: 0.7816\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1589 - accuracy: 0.7816\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1452 - accuracy: 0.7816\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1388 - accuracy: 0.7816\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1334 - accuracy: 0.7759\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1247 - accuracy: 0.7874\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1131 - accuracy: 0.7989\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1073 - accuracy: 0.7989\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0971 - accuracy: 0.7989\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0907 - accuracy: 0.7989\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0821 - accuracy: 0.7989\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0748 - accuracy: 0.7989\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0686 - accuracy: 0.7989\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0611 - accuracy: 0.7989\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0549 - accuracy: 0.7989\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0495 - accuracy: 0.7989\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0437 - accuracy: 0.7989\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0369 - accuracy: 0.7989\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0320 - accuracy: 0.8046\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0274 - accuracy: 0.8046\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0201 - accuracy: 0.8103\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0151 - accuracy: 0.8103\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0100 - accuracy: 0.8103\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0095 - accuracy: 0.8046\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0351 - accuracy: 0.7989\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0340 - accuracy: 0.7989\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0192 - accuracy: 0.8046\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0229 - accuracy: 0.7931\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0057 - accuracy: 0.7931\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9963 - accuracy: 0.7989\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9799 - accuracy: 0.8103\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9702 - accuracy: 0.8046\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9610 - accuracy: 0.8046\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9705 - accuracy: 0.7989\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9659 - accuracy: 0.8046\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9943 - accuracy: 0.7874\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9846 - accuracy: 0.7931\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0225 - accuracy: 0.7759\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0103 - accuracy: 0.7701\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9703 - accuracy: 0.8046\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9775 - accuracy: 0.7931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1745300b280>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50,batch_size=32,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e8a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.9427 - accuracy: 0.7874\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.9370 - accuracy: 0.8046\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9185 - accuracy: 0.8103\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9077 - accuracy: 0.8161\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9230 - accuracy: 0.8161\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9124 - accuracy: 0.8046\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9028 - accuracy: 0.8161\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8871 - accuracy: 0.8333\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8969 - accuracy: 0.8046\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8948 - accuracy: 0.8046\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9058 - accuracy: 0.7931\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8737 - accuracy: 0.8103\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8617 - accuracy: 0.8218\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8601 - accuracy: 0.8161\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8798 - accuracy: 0.8161\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8927 - accuracy: 0.7874\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9014 - accuracy: 0.7816\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8992 - accuracy: 0.8103\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8663 - accuracy: 0.8218\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9326 - accuracy: 0.8046\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9152 - accuracy: 0.7989\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9861 - accuracy: 0.7529\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0204 - accuracy: 0.7701\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9918 - accuracy: 0.7816\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9589 - accuracy: 0.7816\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9678 - accuracy: 0.7586\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9905 - accuracy: 0.7529\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9697 - accuracy: 0.7759\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9644 - accuracy: 0.7644\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9301 - accuracy: 0.7989\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9007 - accuracy: 0.7874\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8874 - accuracy: 0.7759\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8787 - accuracy: 0.7989\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9151 - accuracy: 0.7874\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8712 - accuracy: 0.7989\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8593 - accuracy: 0.7989\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8421 - accuracy: 0.8046\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8460 - accuracy: 0.7989\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8279 - accuracy: 0.8218\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8067 - accuracy: 0.8276\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8007 - accuracy: 0.8276\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8013 - accuracy: 0.8161\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7908 - accuracy: 0.8333\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8191 - accuracy: 0.8506\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7939 - accuracy: 0.8448\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7753 - accuracy: 0.8333\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7638 - accuracy: 0.8563\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7528 - accuracy: 0.8448\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7498 - accuracy: 0.8448\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7523 - accuracy: 0.8563\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7948 - accuracy: 0.8448\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7669 - accuracy: 0.8563\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7429 - accuracy: 0.8563\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7407 - accuracy: 0.8736\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7170 - accuracy: 0.8851\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7043 - accuracy: 0.8966\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.9023\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6946 - accuracy: 0.8966\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.8966\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6761 - accuracy: 0.9023\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6704 - accuracy: 0.9023\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 0.8966\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6568 - accuracy: 0.8966\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6472 - accuracy: 0.8966\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6440 - accuracy: 0.9023\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6392 - accuracy: 0.9080\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6353 - accuracy: 0.9138\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6330 - accuracy: 0.9080\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6282 - accuracy: 0.9138\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6269 - accuracy: 0.9080\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.6291 - accuracy: 0.9023\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6255 - accuracy: 0.8966\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6396 - accuracy: 0.9023\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6308 - accuracy: 0.9023\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6246 - accuracy: 0.9080\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6184 - accuracy: 0.9080\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6133 - accuracy: 0.9138\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6147 - accuracy: 0.9080\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6079 - accuracy: 0.9138\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6266 - accuracy: 0.8908\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6432 - accuracy: 0.8621\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6490 - accuracy: 0.8908\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6376 - accuracy: 0.8793\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6545 - accuracy: 0.8621\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6351 - accuracy: 0.8678\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.8621\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6270 - accuracy: 0.8506\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6041 - accuracy: 0.8793\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5818 - accuracy: 0.8908\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5784 - accuracy: 0.9023\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5614 - accuracy: 0.8966\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5601 - accuracy: 0.8966\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5572 - accuracy: 0.8736\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.8908\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5450 - accuracy: 0.9023\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5399 - accuracy: 0.9080\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5323 - accuracy: 0.9023\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5360 - accuracy: 0.9023\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5280 - accuracy: 0.9080\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5258 - accuracy: 0.8966\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5217 - accuracy: 0.8966\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5167 - accuracy: 0.9080\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5138 - accuracy: 0.9023\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.9080\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5068 - accuracy: 0.9138\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5042 - accuracy: 0.9023\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.8966\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5028 - accuracy: 0.8966\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.9138\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4962 - accuracy: 0.9023\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4936 - accuracy: 0.9080\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4883 - accuracy: 0.9080\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.8966\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.9080\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.9080\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4784 - accuracy: 0.9080\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4764 - accuracy: 0.9080\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4737 - accuracy: 0.9080\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4717 - accuracy: 0.9080\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4693 - accuracy: 0.9138\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4673 - accuracy: 0.9138\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4654 - accuracy: 0.9138\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.9138\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.9138\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4589 - accuracy: 0.9138\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4569 - accuracy: 0.9195\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.9195\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4532 - accuracy: 0.9195\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4516 - accuracy: 0.9138\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4490 - accuracy: 0.9253\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.9195\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4453 - accuracy: 0.9195\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.9195\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.9195\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4394 - accuracy: 0.9195\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4384 - accuracy: 0.9195\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.9195\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4350 - accuracy: 0.9195\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4330 - accuracy: 0.9195\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4318 - accuracy: 0.9195\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4300 - accuracy: 0.9253\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4281 - accuracy: 0.9310\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4267 - accuracy: 0.9310\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4251 - accuracy: 0.9310\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.9310\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.9310\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4202 - accuracy: 0.9253\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.9195\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4172 - accuracy: 0.9253\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4159 - accuracy: 0.9253\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4139 - accuracy: 0.9253\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4126 - accuracy: 0.9310\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4110 - accuracy: 0.9310\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4095 - accuracy: 0.9310\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4076 - accuracy: 0.9310\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4064 - accuracy: 0.9310\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.9310\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4031 - accuracy: 0.9310\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4014 - accuracy: 0.9368\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3998 - accuracy: 0.9368\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3980 - accuracy: 0.9368\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3963 - accuracy: 0.9368\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.9368\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3931 - accuracy: 0.9368\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3911 - accuracy: 0.9368\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3903 - accuracy: 0.9310\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.9310\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3866 - accuracy: 0.9368\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3847 - accuracy: 0.9368\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3832 - accuracy: 0.9310\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.9368\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3798 - accuracy: 0.9368\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3781 - accuracy: 0.9310\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3764 - accuracy: 0.9310\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3746 - accuracy: 0.9368\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.9368\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3723 - accuracy: 0.9425\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3719 - accuracy: 0.9425\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3696 - accuracy: 0.9425\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3685 - accuracy: 0.9368\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.9368\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3753 - accuracy: 0.9195\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.9253\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3715 - accuracy: 0.9310\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3780 - accuracy: 0.9195\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3799 - accuracy: 0.9195\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3842 - accuracy: 0.9195\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3827 - accuracy: 0.9195\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3845 - accuracy: 0.9253\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.9138\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4524 - accuracy: 0.8966\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8851\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8908\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5105 - accuracy: 0.8678\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4804 - accuracy: 0.8506\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.8966\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.8908\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.9080\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.8908\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.9138\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3826 - accuracy: 0.9253\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3731 - accuracy: 0.9310\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3748 - accuracy: 0.9253\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3632 - accuracy: 0.9195\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3544 - accuracy: 0.9253\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3515 - accuracy: 0.9368\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.9253\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.9253\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.9368\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.9253\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.9368\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3522 - accuracy: 0.9310\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5941 - accuracy: 0.8563\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7714 - accuracy: 0.7586\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7081 - accuracy: 0.8103\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7963 - accuracy: 0.7989\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2615 - accuracy: 0.5977\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0421 - accuracy: 0.6782\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5616 - accuracy: 0.4943\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0415 - accuracy: 0.6954\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0766 - accuracy: 0.6954\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9283 - accuracy: 0.7586\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8583 - accuracy: 0.7471\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0433 - accuracy: 0.7471\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1685 - accuracy: 0.6552\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2480 - accuracy: 0.5977\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1908 - accuracy: 0.6609\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1814 - accuracy: 0.7011\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1233 - accuracy: 0.6954\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1136 - accuracy: 0.7299\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0227 - accuracy: 0.7126\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0400 - accuracy: 0.7069\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0019 - accuracy: 0.7184\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9846 - accuracy: 0.7529\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9626 - accuracy: 0.7126\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8556 - accuracy: 0.7816\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8737 - accuracy: 0.7759\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8138 - accuracy: 0.7816\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8888 - accuracy: 0.7644\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7623 - accuracy: 0.8046\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8349 - accuracy: 0.7414\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7110 - accuracy: 0.8161\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7344 - accuracy: 0.8103\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7496 - accuracy: 0.8046\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7225 - accuracy: 0.8103\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6964 - accuracy: 0.8161\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6617 - accuracy: 0.8276\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6198 - accuracy: 0.8333\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6085 - accuracy: 0.8448\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5935 - accuracy: 0.8448\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5727 - accuracy: 0.8621\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5674 - accuracy: 0.8621\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.8621\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5544 - accuracy: 0.8621\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5515 - accuracy: 0.8621\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5295 - accuracy: 0.8736\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5178 - accuracy: 0.8793\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5117 - accuracy: 0.8793\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5347 - accuracy: 0.8736\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5005 - accuracy: 0.8793\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5052 - accuracy: 0.8851\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4828 - accuracy: 0.8851\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.8908\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4680 - accuracy: 0.8966\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4657 - accuracy: 0.8966\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4579 - accuracy: 0.9023\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4537 - accuracy: 0.8966\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.9195\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4485 - accuracy: 0.9138\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.9138\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4462 - accuracy: 0.9195\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4428 - accuracy: 0.9195\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4395 - accuracy: 0.9253\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4358 - accuracy: 0.9253\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.9195\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4376 - accuracy: 0.9253\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4348 - accuracy: 0.9253\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4336 - accuracy: 0.9253\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4326 - accuracy: 0.9253\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.9195\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4265 - accuracy: 0.9195\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.9195\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.9253\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4162 - accuracy: 0.9253\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4182 - accuracy: 0.9195\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4105 - accuracy: 0.9253\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4049 - accuracy: 0.9310\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3999 - accuracy: 0.9253\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3967 - accuracy: 0.9253\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3933 - accuracy: 0.9253\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3910 - accuracy: 0.9368\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3883 - accuracy: 0.9368\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3866 - accuracy: 0.9368\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3846 - accuracy: 0.9310\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3826 - accuracy: 0.9310\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.9368\n",
      "Epoch 297/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3789 - accuracy: 0.9368\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3769 - accuracy: 0.9425\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.9368\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.9368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17400425660>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=300,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6771074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0876 - accuracy: 0.9828\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0875 - accuracy: 0.9828\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0874 - accuracy: 0.9828\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0874 - accuracy: 0.9828\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0873 - accuracy: 0.9828\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0872 - accuracy: 0.9828\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0871 - accuracy: 0.9828\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0870 - accuracy: 0.9828\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0869 - accuracy: 0.9828\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9828\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0867 - accuracy: 0.9828\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0867 - accuracy: 0.9828\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0866 - accuracy: 0.9828\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9828\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9828\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0863 - accuracy: 0.9828\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0863 - accuracy: 0.9828\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0862 - accuracy: 0.9828\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0861 - accuracy: 0.9828\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0861 - accuracy: 0.9828\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0860 - accuracy: 0.9828\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9828\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0858 - accuracy: 0.9828\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0858 - accuracy: 0.9828\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9828\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9828\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0855 - accuracy: 0.9828\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9828\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0854 - accuracy: 0.9828\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9828\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0852 - accuracy: 0.9828\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0852 - accuracy: 0.9828\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0851 - accuracy: 0.9828\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.9828\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.9828\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0849 - accuracy: 0.9828\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9828\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0848 - accuracy: 0.9828\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9828\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9828\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9828\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9828\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9828\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0844 - accuracy: 0.9828\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9828\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9828\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0842 - accuracy: 0.9828\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 0.9828\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9828\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9828\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0839 - accuracy: 0.9828\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9828\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0838 - accuracy: 0.9828\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0837 - accuracy: 0.9828\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9828\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9828\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.9885\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0834 - accuracy: 0.9828\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0835 - accuracy: 0.9828\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0833 - accuracy: 0.9828\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0832 - accuracy: 0.9828\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0833 - accuracy: 0.9828\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0833 - accuracy: 0.9828\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9828\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9828\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0830 - accuracy: 0.9828\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.9828\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9828\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9828\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9828\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9828\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9828\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0824 - accuracy: 0.9828\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0822 - accuracy: 0.9828\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.9828\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9828\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9828\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0821 - accuracy: 0.9828\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9828\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9828\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9828\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9828\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9828\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9828\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9828\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0815 - accuracy: 0.9828\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9828\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0814 - accuracy: 0.9828\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9828\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0812 - accuracy: 0.9828\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9828\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 0.9828\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0810 - accuracy: 0.9828\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9828\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9828\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9828\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9828\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0807 - accuracy: 0.9828\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0807 - accuracy: 0.9828\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9828\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9828\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0805 - accuracy: 0.9828\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.9828\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0803 - accuracy: 0.9828\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0802 - accuracy: 0.9828\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 0.9828\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9828\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9828\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0798 - accuracy: 0.9828\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9828\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9828\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0796 - accuracy: 0.9828\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9828\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0794 - accuracy: 0.9828\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0793 - accuracy: 0.9828\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9828\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0792 - accuracy: 0.9828\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9828\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9828\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9828\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0789 - accuracy: 0.9828\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0789 - accuracy: 0.9828\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9828\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9828\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9828\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0786 - accuracy: 0.9828\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0786 - accuracy: 0.9828\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9828\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9828\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9828\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9828\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9828\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0781 - accuracy: 0.9828\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0781 - accuracy: 0.9828\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9828\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9828\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9828\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9828\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0777 - accuracy: 0.9828\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9828\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9828\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0774 - accuracy: 0.9828\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0773 - accuracy: 0.9828\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0772 - accuracy: 0.9828\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9828\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9828\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9828\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0770 - accuracy: 0.9828\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0769 - accuracy: 0.9828\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0769 - accuracy: 0.9828\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9828\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9828\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9828\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9828\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 0.9828\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9828\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9828\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9828\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9828\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9828\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9828\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9828\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9828\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9828\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9828\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9828\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9828\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0758 - accuracy: 0.9828\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9828\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0757 - accuracy: 0.9828\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0756 - accuracy: 0.9828\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9828\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9828\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9828\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0753 - accuracy: 0.9828\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9828\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9828\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9828\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0751 - accuracy: 0.9828\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0751 - accuracy: 0.9828\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9828\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9828\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0749 - accuracy: 0.9828\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9885\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9828\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9828\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9828\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9828\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9828\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.9828\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9828\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9828\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0743 - accuracy: 0.9828\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0742 - accuracy: 0.9828\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0742 - accuracy: 0.9828\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9828\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9828\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9828\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9828\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1742541f550>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200\n",
    "          ,batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd0726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eb00cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('chatbotdeepfake2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11c22109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('chatbotdeepfake2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a81c7c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you: hello\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "28\n",
      "greeting\n",
      "chatbot: Hello! How can I assist you today?\n",
      "you: how r you\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "55\n",
      "whatsup\n",
      "chatbot: All good..What about you?\n",
      "you: who made u\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "43\n",
      "programmer\n",
      "chatbot: I was made by DeepGuard developers.\n",
      "you: lol\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "30\n",
      "haha\n",
      "chatbot: Glad I could make you laugh !\n",
      "you: idiot\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "33\n",
      "insult\n",
      "chatbot: Well that hurts :(\n",
      "you: ty\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "48\n",
      "suggest\n",
      "chatbot: Please mail your suggestions to Pixel.Deepguard.suggestions@DeepGuard application Thank you for helping me improve!\n",
      "you: cya bye\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "30\n",
      "haha\n",
      "chatbot: Glad I could make you laugh !\n",
      "you: see you bye\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "27\n",
      "goodbye\n",
      "chatbot: Farewell! Feel free to return whenever you have more inquiries.\n",
      "you: what is computer vision?\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "9\n",
      "cv\n",
      "chatbot: Computer Vision (CV) is a field of artificial intelligence that focuses on enabling machines to analyze, understand, and interpret visual information from images or videos. Key points about Computer Vision include:\n",
      "you: what is natural language ?\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "39\n",
      "nlp\n",
      "chatbot: Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language. NLP enables computers to understand, interpret, and generate human language. Key points about Natural Language Processing include:\n",
      "you: bye\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "27\n",
      "goodbye\n",
      "chatbot: See you later! Take care and stay curious.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     text_p \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m     prediction_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myou: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     text_p\u001b[38;5;241m.\u001b[39mappend(prediction_input)\n\u001b[0;32m      6\u001b[0m     prediction_input \u001b[38;5;241m=\u001b[39m tokenize_data(tokenizer, text_p)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[1;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "while True:\n",
    "    text_p = []\n",
    "    prediction_input = input('you: ')\n",
    "    text_p.append(prediction_input)\n",
    "    prediction_input = tokenize_data(tokenizer, text_p)\n",
    "    prediction_input = np.array(prediction_input).reshape(-1)\n",
    "    prediction_input = pad_sequences([prediction_input], 16)\n",
    "    prediction = model.predict(prediction_input)\n",
    "    output = prediction.argmax()\n",
    "    print(output)\n",
    "    response_tag = le.inverse_transform([output])[0]\n",
    "    print(response_tag)\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        if intent['tag'] == response_tag:\n",
    "            responses = intent['responses']\n",
    "            print('chatbot:', random.choice(responses))\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671029a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
